---
title: 'PHP 2514 HW #5'
author: "Blain Morin"
date: "April 16, 2018"
output: html_document
---

```{r}
library(arm)
library(metRology)
library(knitr)
```

# Question 1: Gellman Hill Chapter 7 #2

According to the textbook, 52% of the United States is female. So, we will run our simulation using this proportion. 

```{r}

### Choose our number of simulations:
n.sims = 1000000

### Initiate a results vector:
totalweight = c()


### Run simulation using a for loop:


for (i in 1:n.sims) {
  
  sex = rbinom(10, 1, .52) ### Determines gender of the ten people (1 = Female)
  
  weight = ifelse(sex == 1, 
                  rnorm(10, mean = 4.96, sd = .2), ### Weight distribution for female 
                  rnorm(10, mean = 5.13, sd = .17)) ### Weight distribution for male
  
  result = sum(exp(weight)) ### Get weight in pounds by exponentiating and add them all together
  
  totalweight[i] = result ### Tally our simulation results
  
}

breaks = sum(totalweight >1750) ### Count the number of cases where the elevator breaks

breaks / n.sims ### Probability of failure 

```

After simulating a million times, we find that the probability that the elevator breaks is .05.

## 2: Gelman Hill Chapter 7 #8

### (a)

How do we use degrees freedom??????????????????

```{r}

### Set number of simulations to 1000
n.sims = 1000

### Set estimates given in the problem
u.cost = 600




```


## 3: Gelman Hill Chapter 8 #1

### (a)

```{r}



x1 = 1:100
x2 = rbinom(100, 1, .5)
y = 3 + .1 * x1 + .5 * x2 + rnorm(100, 0, 1)

model.3a = lm(y ~ x1 + x2)
summary.3a = summary(model.3a)
betas.3a = summary.3a$coefficients[ , 1]
beta.error.3a = summary.3a$coefficients[ , 2]
confidence.68 = cbind(betas.3a - beta.error.3a, betas.3a + beta.error.3a)

confidence.68

```


In this run, the true paramters are all within the 68% intervals.

### (b)

```{r}

### Set number of simulations to 1000
n.sims = 1000

### Initialize a matrix for the parameter estimates
b.cover = matrix(nrow = 1000, ncol = 3)

### Run the simulation
for (i in 1:n.sims) {
  
  b.x1 = 1:100
  b.x2 = rbinom(100, 1, .5)
  y.b = 3 + .1 * b.x1 + .5 * b.x2 + rnorm(100, 0, 1)
  b.model = lm(y.b ~ b.x1 +b.x2)
  summary.3b = summary(b.model)
  est.3b = summary.3b$coefficients[ , 1]
  err.3b = summary.3b$coefficients[ , 2]
  
  ### Set the 68% bounds
  lower.bound = est.3b - err.3b
  upper.bound = est.3b + err.3b
  
  ### These statements test if the true parameters are within the bounds
  int.b = ifelse(3 >= lower.bound[1] && 3 <= upper.bound[1], TRUE, FALSE)
  x1.beta = ifelse(.1 >= lower.bound[2] && .1 <= upper.bound[2], TRUE, FALSE)
  x2.beta = ifelse(.5 >= lower.bound[3] && .5 <= upper.bound[3], TRUE, FALSE)
  
  ### Add the test results to the coverage matrix
  b.cover[i , 1] = int.b
  b.cover[i , 2] = x1.beta
  b.cover[i , 3] = x2.beta
  
}

### The coverage probability is the mean
inter.prob = mean(b.cover[ , 1])
x1.prob = mean(b.cover[ , 2])
x2.prob = mean(b.cover[, 3])

### Report results in a table
probs = cbind(inter.prob, x1.prob, x2.prob)

kable(probs)


```

Thus, the probability that the 68% confidence interval covers the true parameter is .68 for the intercept, .67 for the beta on x1, and .69 for the beta on x2. These are all very close to the expected probability of .68.

### (c)

```{r}

### Set number of simulations to 1000
n.sims = 1000

### Initialize a matrix for the parameter estimates
c.cover = matrix(nrow = 1000, ncol = 3)

### Run the simulation
for (i in 1:n.sims) {
  
  c.x1 = 1:100
  c.x2 = rbinom(100, 1, .5)
  y.c = 3 + .1 * c.x1 + .5 * c.x2 + rt.scaled(100, df = 4, mean = 0, sd = 5)
  c.model = lm(y.c ~ c.x1 +c.x2)
  summary.3c = summary(c.model)
  est.3c = summary.3c$coefficients[ , 1]
  err.3c = summary.3c$coefficients[ , 2]
  
  ### Set the 68% bounds
  lower.bound = est.3c - err.3c
  upper.bound = est.3c + err.3c
  
  ### These statements test if the true parameters are within the bounds
  int.c = ifelse(3 >= lower.bound[1] && 3 <= upper.bound[1], TRUE, FALSE)
  x1.c = ifelse(.1 >= lower.bound[2] && .1 <= upper.bound[2], TRUE, FALSE)
  x2.c = ifelse(.5 >= lower.bound[3] && .5 <= upper.bound[3], TRUE, FALSE)
  
  ### Add the test results to the coverage matrix
  c.cover[i , 1] = int.c
  c.cover[i , 2] = x1.c
  c.cover[i , 3] = x2.c
  
}

### The coverage probability is the mean
inter.prob.c = mean(c.cover[ , 1])
x1.prob.c = mean(c.cover[ , 2])
x2.prob.c = mean(c.cover[, 3])

### Report results in a table
probs.c = cbind(inter.prob.c, x1.prob.c, x2.prob.c)

kable(probs.c)


```

All of the coverage probabilities are near .68. Thus, having errors with a t distribution does not affect our coverge probabilities. 

## 5: Gelman Hill Chapter 11 #4

### (a)

```{r, message=FALSE}

library(readr)
library(lubridate)
library(ggplot2)
library(dplyr)

### Load data and change the string to a date
cd4 = read_csv("cd4.csv")
cd4$vdate = mdy(cd4$vdate)
cd4.complete = cd4[complete.cases(cd4),]

### Create the plot
cd4.complete %>%
  ggplot(aes(x = vdate, y = cd4pct**.5)) + geom_line(aes(color = as.factor(newpid)), alpha = .8)+ ylab("sqrt(cd4%)") +
  xlab("Time") + theme_classic() + theme(legend.position = "none") + ggtitle("Sqrt CD4 over Time (by person)") +
  theme(panel.background = element_rect(fill = "gray4")) 


```


### (b)

```{r}

cd4.complete %>%
  ggplot(aes(x = vdate, y = cd4pct**.5)) +
  geom_smooth(method = "lm", se = FALSE, aes(color = as.factor(newpid)), alpha = .6) +
  geom_point(aes(color = as.factor(newpid)), alpha = .5, size = .5) +
  theme_classic() + theme(panel.background = element_rect(fill = "gray4")) +
  theme(legend.position = "none") + 
  ylab("sqrt(cd4pct)") +
  xlab("Time") +
  ggtitle("Linear Models by Person")


```

### (c)

```{r}

library(lattice)
library(lme4)
library(sjPlot)
attach(cd4.complete)

###remove conflicting package
detach(package:metRology)

### Run no pooling regressions for each individual
no.pool = lmList(cd4pct**.5 ~ vdate | newpid, data = cd4.complete)

detach(cd4.complete)

### Condense data so we have one line per individual
cd4.slice = cd4.complete %>% group_by(newpid) %>% slice(1)

### Combine into df with slope, intercepts, baseage, and treatment
no.pool.df = as.data.frame(coef(no.pool))
no.pool.df = cbind(no.pool.df, cd4.slice$baseage)
no.pool.df = cbind(no.pool.df, as.factor(cd4.slice$treatmnt))
names(no.pool.df) = c("intercept", "slope", "baseage", "treatmnt")
attach(no.pool.df)

### Run model for intercept
intercept.model = lm(intercept ~ baseage + treatmnt)

### Run model for slope
slope.model = lm(slope ~ baseage + treatmnt)

sjt.lm(intercept.model, slope.model)
detach(no.pool.df)
```

We see from the above table that base age and treatment group do not have a significant effect on the baseline cd4 percentage. Moreover, neither base age or treatment has an effect on the change in cd4 during the study.

## 6: Gelman Hill Chapter 12 #2:

### (a)

```{r, message = FALSE}

attach(cd4.complete)

vary.intercept = lmer(cd4pct ~ 1 + vdate + (1 | newpid))
sjt.lmer(vary.intercept)

```

The coefficient for time is -0.01. This result means that for each additional day, average cd4 percentage decreases by .01.

### (b)

```{r, message=FALSE}

vary.intercept2 = lmer(cd4pct ~ 1 + baseage + as.factor(treatmnt) + vdate + (1|newpid))
sjt.lmer(vary.intercept2)

```

The interpretation for time is the same as in part (a). For every year in base age, the initial cd4 percentage decreases by .96. The beta on treatment group means that relative to treatment group 1, treatment group 2 sees an average cd4 percentage that is initially 1.91 points higher. 

### (c)

```{r, message = FALSE}

sjt.lmer(vary.intercept, vary.intercept2)



```

### (d)

```{r, message = FALSE}

anova(vary.intercept, vary.intercept2)


```

Since our p value is .01, there is evidence to suggest that the more complicated model with the group level predictors gives us a better fit. 

## 7: Gelman Hill Chapter 12 #3

### (a)

```{r}

### Going to predict for a point that is one month from the max date

max(vdate)

cd4.slice.pred = cd4.slice
cd4.slice.pred$vdate = as_date("1991-02-14")

preds = predict(vary.intercept2, cd4.slice.pred)

head(preds)
summary(preds)

```

Since there is no bound at 0, some of our predictions are negative. This may suggest that using a logistic regression would be a better model.

### (b)

```{r}
detach(cd4.complete)

### Create a data frame with all possible combinations of independent variables, for each time period

combos = data.frame(vdate = rep(unique(cd4.slice$vdate), each = 2))
combos = cbind(combos, baseage = 4)
combos = cbind(combos, treatmnt = as.factor(rep(1:2)))
combos = cbind(combos, newpid = rep(1:2))

### Predict for each row, this gives us a prediction for a 4 year old child
### for every date and treatment group

preds2 = predict(vary.intercept2, combos)

head(preds2)
summary(preds2)
```

